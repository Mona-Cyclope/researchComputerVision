{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOTTOM UP PYRAMID\n",
    "\n",
    "## Architecture \n",
    "\n",
    "The architecture is composed of 2 modules.\n",
    "* features pyramid (backbone)\n",
    "* classifier (head)\n",
    "  \n",
    "The features pyramid is build bottom up (BUP) meanning that the features are first extracted at the biggest scale (smaller image first).\n",
    "The feature extracted at this scale are fed to the head to output a prediction.\n",
    "The same procedure is repeated for the next smaller scale (bigger image) however the features extracted at the immediately bigger scale is reused via a residual connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.w0 = nn.Conv2d(head_features, head_features, kernel_size=1)\n",
    "        self.w1 = nn.Conv2d(head_features, head_features, kernel_size=1)\n",
    "        self.wO = nn.Conv2d(head_features, num_classes, kernel_size=1)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x0 = self.act(self.w0(x)) + x\n",
    "        x1 = self.act(self.w1(x0)) + x0\n",
    "        y = self.w0(x1)\n",
    "        return y\n",
    "\n",
    "class BUP(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, num_classes, n_steps=4, patch_features=32, head_features=128, patch_size=7):\n",
    "        \n",
    "        super().__init__()\n",
    "        conv0 = nn.Conv2d(input_channels, input_channels, kernel_size=3, padding=1, groups=input_channels) \n",
    "        conv1 = nn.Conv2d(input_channels, patch_features, kernel_size=3, padding=0) \n",
    "        batch_norm = nn.BatchNorm2d(input_channels)\n",
    "        act = nn.LeakyReLU(inplace=False)\n",
    "        self.conv_in = nn.Sequential(batch_norm, conv0, act, conv1)\n",
    "        self.n_steps = n_steps\n",
    "        patch_batch_norm = nn.BatchNorm2d(head_features)\n",
    "        self.patcher = nn.Sequential( act, nn.Conv2d(patch_features, head_features, kernel_size=patch_size, stride=patch_size), act, patch_batch_norm)\n",
    "        self.upsample = nn.UpsamplingNearest2d()\n",
    "        self.head = Head(head_features, num_classes)\n",
    "        \n",
    "    def forward_scale_i(self, x, scale, f_res=None, p_res=None, y_res=None):\n",
    "        x_i = x[:,:,::scale,::scale]\n",
    "        f_i = self.conv_in(x_i)\n",
    "        if f_res is not None: \n",
    "            self.upsample.size = f_i.shape[2:]\n",
    "            f_i = f_i + self.upsample(f_res)\n",
    "        p_i = self.patcher(f_i) \n",
    "        if p_res is not None:\n",
    "            self.upsample.size = p_i.shape[2:]\n",
    "            p_i  = p_i + self.upsample(p_res)\n",
    "        y_i = self.head(p_i) \n",
    "        if y_res is not None:\n",
    "            self.upsample.size = y_i.shape[2:]\n",
    "            y_i = y_i + self.upsample(y_res)\n",
    "        return f_i, p_i, y_i\n",
    "    \n",
    "    def forward(self,x,stop_step=1):\n",
    "        n_steps = self.n_steps\n",
    "        f_n, p_n, y_n = [], [], []\n",
    "        for s in range(n_steps, stop_step-1, -1):\n",
    "            x_i = x[:,:,::s,::s]\n",
    "            if len(f_n) == 0:\n",
    "                f_i, p_i, y_i = self.forward_scale_i(x_i, s)\n",
    "                f_n, p_n, y_n = [f_i], [p_i], [y_i]\n",
    "            else:\n",
    "                f_i, p_i, y_i = self.forward_scale_i(x_i, s, f_res=f_n[-1], p_res=p_n[-1], y_res=y_n[-1])\n",
    "                f_n += [f_i]\n",
    "                p_n += [p_i]\n",
    "                y_n += [y_i]\n",
    "        return y_n     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# define the LightningModule\n",
    "class ClassifierModule(pl.LightningModule):\n",
    "    def __init__(self, *args,**kwargs):\n",
    "        super().__init__()\n",
    "        self.model = BUP(*args, **kwargs)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        y_n = self.model(x)\n",
    "        acc_loss = 0.0\n",
    "        for y_i in y_n:\n",
    "            loss = nn.functional.cross_entropy(y_i, y[:,None,None].expand_as(y_i[:,0]), reduce=False)\n",
    "            acc_loss += nn.functional.adaptive_max_pool2d(loss,1)[:,0,0].mean()/len(y_n)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"train_loss\", acc_loss)\n",
    "        return acc_loss\n",
    "    \n",
    "    def validating_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_n = self.model(x, stop_step=self.model.n_steps)\n",
    "        acc_loss = 0.0\n",
    "        for y_i in y_n:\n",
    "            loss = nn.functional.cross_entropy(y_i, y[:,None,None].expand_as(y_i[:,0]), reduce=False)\n",
    "            acc_loss += nn.functional.adaptive_max_pool2d(loss,1)[:,0,0].mean()/len(y_n)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log(\"valid_loss\", acc_loss)\n",
    "        return acc_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as ds\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "train_dataset = ds.STL10('./datasets', split='train', download=True, transform=transforms.Compose([transforms.RandAugment(), transforms.ToTensor()]))\n",
    "test_dataset = ds.STL10('./datasets', split='test', download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32)\n",
    "valid_dataloader = DataLoader(test_dataset, batch_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | BUP  | 236 K \n",
      "-------------------------------\n",
      "236 K     Trainable params\n",
      "0         Non-trainable params\n",
      "236 K     Total params\n",
      "0.945     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27:  76%|███████▋  | 120/157 [00:07<00:02, 16.04it/s, loss=1.69, v_num=4]"
     ]
    }
   ],
   "source": [
    "mod = ClassifierModule(3, 10, n_steps=3)\n",
    "trainer = pl.Trainer( max_epochs=100, accelerator=\"gpu\", devices=[0])\n",
    "trainer.fit(model=mod, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch_dl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b58258dc28d9facf74b6818431509829ffeb0724e2c31fdc9669359a66a0748"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
